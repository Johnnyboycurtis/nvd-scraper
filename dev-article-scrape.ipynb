{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Vendor Articles from CVEs\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"my bar!\")\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "\n",
    "# model = SentenceTransformer('stsb-roberta-large') # deprecated\n",
    "model = SentenceTransformer('sentence-transformers/all-distilroberta-v1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test = pd.DataFrame(np.random.randint(0, 100, (100000, 6)))\n",
    "\n",
    "#test.progress_apply(lambda x: x.sum(), axis=1).shape[0] == test.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "spaces_pattern = re.compile('\\s+') # remove new lines \\n and \\t\n",
    "\n",
    "URL = \"https://www.jenkins.io/security/advisory/2023-06-14/\"\n",
    "\n",
    "headers = {\n",
    "   \"User-Agent\": \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:15.0) Gecko/20100101 Firefox/15.0.1\",\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "def request_articles(url_list):\n",
    "    soup_list = []\n",
    "    for url in url_list:\n",
    "        soup = beautiful_request(url)\n",
    "        if soup:\n",
    "            article_text_list = text_prep(soup)\n",
    "            soup_list.append(article_text_list)\n",
    "    return request_articles\n",
    "\n",
    "\n",
    "def beautiful_request(url):\n",
    "    # sometimes you'll get a ConnectionError\n",
    "    try:\n",
    "        # url = 'https://github.com/syz913/CVE-reports/blob/main/CVE-2023-31821.md'\n",
    "        if 'github.com' in url:\n",
    "            url += '?raw=true'\n",
    "            response = requests.get(url = url, timeout=(4, 5), allow_redirects=True, headers = headers)\n",
    "        else:\n",
    "            response = requests.get(url = url, timeout=(4, 5), allow_redirects=False, headers=headers)\n",
    "    except:\n",
    "        response = None\n",
    "    if (response is not None) and (response.status_code in range(200, 300)):\n",
    "        # Before passing response.text to Beautifulsoup\n",
    "        # we need to remove any \\n that are within sentences and\n",
    "        # unrelated to HTML new lines\n",
    "        htmltext = response.text.replace('.\\n', '...') # prevent sentences from splitting\n",
    "        soup = BeautifulSoup(htmltext, 'lxml')\n",
    "        htmltext = soup.text.split('\\n')\n",
    "        # does this after is trickier...or at least requires more thought\n",
    "        # this is faster for now...\n",
    "        return htmltext\n",
    "    return None\n",
    "\n",
    "\n",
    "def spaces_filter(text_list):\n",
    "    # ['this', ' ', ' ', 'that', 'this']\n",
    "    return len(text_list) > 0\n",
    "\n",
    "\n",
    "def word_count(text):\n",
    "    text_list = text.split(' ') # get rid of extra spaces so they don't count\n",
    "    text_list = list(filter(spaces_filter, text_list))\n",
    "    return len(text_list)\n",
    "\n",
    "\n",
    "def word_count_filter(text):\n",
    "    n = word_count(text)\n",
    "    return n > 3\n",
    "\n",
    "\n",
    "def text_prep(soup):\n",
    "    # we get all this html code, let's just grab the text\n",
    "    text = soup.text.split('\\n')\n",
    "    #text # we get a lot of empty junk, so filter it out\n",
    "    text = list(filter(spaces_filter, text))\n",
    "    text = list(filter(word_count_filter, text)) # should I combine this with the above?\n",
    "    text_list = text\n",
    "    return text_list\n",
    "\n",
    "\n",
    "def remove_newlines(text):\n",
    "    # remove \\n and \\t\n",
    "    text = re.sub(spaces_pattern, ' ', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def find_text_similarity(text_list, embedding1=None, nvd_description=None):\n",
    "    if nvd_description:\n",
    "        embedding1 = model.encode(nvd_description, convert_to_tensor=True)\n",
    "    scores = []\n",
    "    for line in tqdm(text_list):\n",
    "        embedding2 = model.encode(line, convert_to_tensor=True)\n",
    "        # compute similarity scores of two embeddings\n",
    "        cosine_scores = util.pytorch_cos_sim(embedding1, embedding2)\n",
    "        scores.append(float(cosine_scores))\n",
    "    return scores\n",
    "\n",
    "\n",
    "def fetch_relevant_text(scores, text_list):\n",
    "    idx = pd.Series(scores).idxmax()\n",
    "    # fetch the relevant text\n",
    "    rel_text = text_list[max(0, idx-2):min(len(text_list), idx+3)]\n",
    "    return rel_text, idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(row):\n",
    "    scraped_vendor_articles = []\n",
    "    nvd_description = row['description']\n",
    "    embedding_description = model.encode(nvd_description, convert_to_tensor=True)\n",
    "    articles = map(beautiful_request, row['references_url_list'])\n",
    "    articles = list(filter(lambda art: art is not None, articles))\n",
    "\n",
    "    for article_text_list in articles:\n",
    "        if len(article_text_list) < 400:\n",
    "            scores = find_text_similarity(article_text_list, embedding1=embedding_description)\n",
    "            relevant_text, _ = fetch_relevant_text(scores, article_text_list)\n",
    "            vendor_article = \"...\".join(relevant_text) # this is one complete article from one web page\n",
    "            scraped_vendor_articles.append(vendor_article)\n",
    "        else:\n",
    "            print(\"too fucking long...\")\n",
    "\n",
    "    scraped_vendor_articles = \" --- \".join(scraped_vendor_articles)\n",
    "    scraped_vendor_articles = remove_newlines(scraped_vendor_articles) # remove \\t, \\s, \\r, \\n\n",
    "    return scraped_vendor_articles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/nvd_cve_metrics.txt\", sep=\"|\", index_col=0).sample(400, random_state=456)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['references_url_list'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['references_url_list'] = df['references_url_list'].fillna('[]').apply(eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by='publishedDate', ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "startTime = dt.datetime.now()\n",
    "\n",
    "for i, row in df.head(50).iterrows():\n",
    "    output = main(row)\n",
    "    results.append(output)\n",
    "    print(i, dt.datetime.now() - startTime)\n",
    "    print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row['references_url_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#requests.get('http://www.securityfocus.com/bid/49500')\n",
    "#response = requests.get('http://www.redhat.com/support/errata/RHSA-2011-1249.html', allow_redirects=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get('https://www.unisoc.com/en_us/secy/announcementDetail/1676902764208259073', allow_redirects=False)\n",
    "response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "   \"User-Agent\": \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:15.0) Gecko/20100101 Firefox/15.0.1\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(url = 'https://www.unisoc.com/en_us/secy/announcementDetail/1676902764208259073', timeout=(4, 5), allow_redirects=False, headers=headers)\n",
    "response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('stuff.html', 'w+') as mf:\n",
    "    mf.writelines(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "soup.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "htmltext = beautiful_request('https://www.unisoc.com/en_us/secy/announcementDetail/1676902764208259073')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(soup.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = '   **Vulnerability name:**  Exposure of secret in ALBIS\\r'\n",
    "remove_newlines(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = soup.prettify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "htmltext = out.split('\\n')\n",
    "htmltext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, line in enumerate(htmltext):\n",
    "    if 'CVE-2023-30932' in line:\n",
    "        print(i, line)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# syz913/CVE-reports/blob/main/CVE-2023-31821.md\n",
    "url = 'https://raw.githubusercontent.com/syz913/CVE-reports/main/CVE-2023-31821.md?raw=true'\n",
    "response = requests.get(url = url, timeout=(4, 5), allow_redirects=False)\n",
    "response.text\n",
    "# https://github.com/syz913/CVE-reports/blob/main/CVE-2023-31821.md?raw=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://github.com/syz913/CVE-reports/blob/main/CVE-2023-31821.md'\n",
    "if 'github.com' in url:\n",
    "    url += '?raw=true'\n",
    "    response = requests.get(url = url, timeout=(4, 5), allow_redirects=True)\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
